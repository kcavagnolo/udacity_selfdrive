{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout2D, ELU\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Cropping2D\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  9384\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "reduced = False\n",
    "\n",
    "if reduced == True:\n",
    "    csv_filepath = 'data-udacity/driving_log_reduced.csv'\n",
    "else:\n",
    "    csv_filepath = 'data-udacity/driving_log.csv'\n",
    "samples = []\n",
    "with open(csv_filepath) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "        \n",
    "def add_to_samples(csv_filepath, samples):\n",
    "    with open(csv_filepath) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            samples.append(line)\n",
    "    return samples\n",
    "\n",
    "samples = add_to_samples('data-recovery-annie/driving_log.csv', samples)\n",
    "\n",
    "samples = add_to_samples('data-udacity-flipped/driving_csv_flipped_headoff.csv)\n",
    "\n",
    "# samples = add_to_samples('data-extra-laps/driving_log.csv', samples)\n",
    "\n",
    "# samples = add_to_samples('data-recovery/driving_log.csv', samples)\n",
    "\n",
    "# samples = add_to_samples('data-mouse/driving_log.csv', samples)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "samples = add_to_samples('data-udacity/driving_log_18_bridge_reduced2.csv)\n",
    "\"\"\"\n",
    "samples = samples[1:]\n",
    "print(\"Samples: \", len(samples))    \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = './data-udacity/'+batch_sample[0]\n",
    "                # name = './data-udacity/IMG/'+batch_sample[0].split('/')[-1]\n",
    "                center_image = mpimg.imread(name)\n",
    "                center_angle = float(batch_sample[3])\n",
    "                images.append(center_image)\n",
    "                angles.append(center_angle)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            \n",
    "            # print(\"X_train: \", X_train)\n",
    "            # print(\"y_train: \", y_train)\n",
    "            yield shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize(image):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 66, 200)\n",
    "\n",
    "def resize_comma(image):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 40, 160)\n",
    "\n",
    "def resize_comma_10_40(image):\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 40, 160)\n",
    "\n",
    "def halve_size(image):\n",
    "    imshape = image.shape\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, imshape[0] // 2, imshape[1] // 2)\n",
    "\n",
    "def resize_80_160(image):\n",
    "    imshape = image.shape\n",
    "    import tensorflow as tf  # This import is required here otherwise the model cannot be loaded in drive.py\n",
    "    return tf.image.resize_images(image, 80, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "cropping2d_7 (Cropping2D)        (None, 65, 320, 3)    0           cropping2d_input_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)               (None, 40, 160, 3)    0           cropping2d_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)               (None, 40, 160, 3)    0           lambda_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 10, 40, 16)    3088        lambda_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_36 (ELU)                     (None, 10, 40, 16)    0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 5, 20, 32)     12832       elu_36[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_37 (ELU)                     (None, 5, 20, 32)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 3, 10, 64)     51264       elu_37[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 1920)          0           convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "elu_38 (ELU)                     (None, 1920)          0           flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 512)           983552      elu_38[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_39 (ELU)                     (None, 512)           0           dense_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             513         elu_39[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 1051249\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Comma.ai model\n",
    "# https://github.com/commaai/research/blob/master/train_steering_model.py\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Crop 50 pixels from the top of the image and 20 from the bottom\n",
    "model.add(Cropping2D(cropping=((70, 25), (0, 0)),\n",
    "                     dim_ordering='tf', # default\n",
    "                     input_shape=(160, 320, 3)))\n",
    "\n",
    "# Resize the data\n",
    "model.add(Lambda(resize_comma))\n",
    "\n",
    "# model.add(Lambda(lambda x: x[:,:,:,0:1]))\n",
    "\n",
    "model.add(Lambda(lambda x: (x/255.0) - 0.5))\n",
    "\n",
    "model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(.2))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(512))\n",
    "# model.add(Dropout(.5))\n",
    "model.add(ELU())\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "\n",
    "model.compile(optimizer=adam, loss=\"mse\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing reference: [Geoff Breemer](https://carnd-forums.udacity.com/questions/36045049/answers/36047341)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "cropping2d_6 (Cropping2D)        (None, 80, 320, 3)    0           cropping2d_input_6[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)               (None, 66, 200, 3)    0           cropping2d_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)               (None, 66, 200, 1)    0           lambda_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)               (None, 66, 200, 1)    0           lambda_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Convolution2D)            (None, 31, 98, 24)    624         lambda_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "elu_27 (ELU)                     (None, 31, 98, 24)    0           conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Convolution2D)            (None, 14, 47, 36)    21636       elu_27[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_28 (ELU)                     (None, 14, 47, 36)    0           conv2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Convolution2D)            (None, 5, 22, 48)     43248       elu_28[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_29 (ELU)                     (None, 5, 22, 48)     0           conv3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv4 (Convolution2D)            (None, 3, 20, 64)     27712       elu_29[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_30 (ELU)                     (None, 3, 20, 64)     0           conv4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv5 (Convolution2D)            (None, 1, 18, 64)     36928       elu_30[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "flatten1 (Flatten)               (None, 1152)          0           conv5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "elu_31 (ELU)                     (None, 1152)          0           flatten1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense1 (Dense)                   (None, 1164)          1342092     elu_31[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_32 (ELU)                     (None, 1164)          0           dense1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense2 (Dense)                   (None, 100)           116500      elu_32[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_33 (ELU)                     (None, 100)           0           dense2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense3 (Dense)                   (None, 50)            5050        elu_33[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_34 (ELU)                     (None, 50)            0           dense3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense4 (Dense)                   (None, 10)            510         elu_34[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "elu_35 (ELU)                     (None, 10)            0           dense4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense5 (Dense)                   (None, 1)             11          elu_35[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 1594311\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# NVIDIA version\n",
    "model = Sequential()\n",
    "\n",
    "# Crop the images\n",
    "model.add(Cropping2D(cropping=((60, 20), (0, 0)),\n",
    "                     dim_ordering='tf', # default\n",
    "                     input_shape=(160, 320, 3)))\n",
    "\n",
    "# Resize the data\n",
    "model.add(Lambda(resize))\n",
    "\n",
    "model.add(Lambda(lambda x: x[:,:,:,0:1]))\n",
    "\n",
    "# Normalise data\n",
    "# TODO: some people use /255.0 - 0.5. Why?\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(24, 5, 5, subsample=(2, 2), border_mode=\"valid\", init='he_normal', name='conv1'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(36, 5, 5, subsample=(2, 2), border_mode=\"valid\", init='he_normal', name='conv2'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(48, 5, 5, subsample=(2, 2), border_mode=\"valid\", init='he_normal', name='conv3'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode=\"valid\", init='he_normal', name='conv4'))\n",
    "model.add(ELU())\n",
    "model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode=\"valid\", init='he_normal', name='conv5'))\n",
    "model.add(Flatten(name='flatten1'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(1164, init='he_normal', name='dense1'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(100, init='he_normal', name='dense2'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(50, init='he_normal', name='dense3'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(10, init='he_normal', name='dense4'))\n",
    "model.add(ELU())\n",
    "model.add(Dense(1, init='he_normal', name='dense5'))\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.01)\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.5765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessica/anaconda/lib/python3.5/site-packages/keras/engine/training.py:1470: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: saving model to ./tmp/comma-v3g.00-0.01.hdf5\n",
      "8462/8445 [==============================] - 44s - loss: 0.0122 - acc: 0.5760 - val_loss: 0.0127 - val_acc: 0.5527\n",
      "Epoch 2/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.5766Epoch 00001: saving model to ./tmp/comma-v3g.01-0.01.hdf5\n",
      "8462/8445 [==============================] - 42s - loss: 0.0111 - acc: 0.5768 - val_loss: 0.0129 - val_acc: 0.5469\n",
      "Epoch 3/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.5773Epoch 00002: saving model to ./tmp/comma-v3g.02-0.01.hdf5\n",
      "8462/8445 [==============================] - 38s - loss: 0.0107 - acc: 0.5773 - val_loss: 0.0125 - val_acc: 0.5448\n",
      "Epoch 4/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.5754Epoch 00003: saving model to ./tmp/comma-v3g.03-0.01.hdf5\n",
      "8462/8445 [==============================] - 36s - loss: 0.0100 - acc: 0.5750 - val_loss: 0.0123 - val_acc: 0.5475\n",
      "Epoch 5/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.5779Epoch 00004: saving model to ./tmp/comma-v3g.04-0.01.hdf5\n",
      "8462/8445 [==============================] - 36s - loss: 0.0097 - acc: 0.5781 - val_loss: 0.0118 - val_acc: 0.5432\n",
      "Epoch 6/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.5745Epoch 00005: saving model to ./tmp/comma-v3g.05-0.01.hdf5\n",
      "8462/8445 [==============================] - 35s - loss: 0.0095 - acc: 0.5747 - val_loss: 0.0123 - val_acc: 0.5327\n",
      "Epoch 7/20\n",
      "8416/8445 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.5745Epoch 00006: saving model to ./tmp/comma-v3g.06-0.01.hdf5\n",
      "8448/8445 [==============================] - 34s - loss: 0.0094 - acc: 0.5747 - val_loss: 0.0114 - val_acc: 0.5454\n",
      "Epoch 8/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.5719Epoch 00007: saving model to ./tmp/comma-v3g.07-0.01.hdf5\n",
      "8462/8445 [==============================] - 34s - loss: 0.0090 - acc: 0.5727 - val_loss: 0.0122 - val_acc: 0.5559\n",
      "Epoch 9/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.5776Epoch 00008: saving model to ./tmp/comma-v3g.08-0.01.hdf5\n",
      "8462/8445 [==============================] - 33s - loss: 0.0087 - acc: 0.5769 - val_loss: 0.0125 - val_acc: 0.5570\n",
      "Epoch 10/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.5765Epoch 00009: saving model to ./tmp/comma-v3g.09-0.01.hdf5\n",
      "8462/8445 [==============================] - 32s - loss: 0.0086 - acc: 0.5762 - val_loss: 0.0124 - val_acc: 0.5633\n",
      "Epoch 11/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.5772Epoch 00010: saving model to ./tmp/comma-v3g.10-0.01.hdf5\n",
      "8462/8445 [==============================] - 32s - loss: 0.0085 - acc: 0.5775 - val_loss: 0.0123 - val_acc: 0.5454\n",
      "Epoch 12/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.5762Epoch 00011: saving model to ./tmp/comma-v3g.11-0.01.hdf5\n",
      "8462/8445 [==============================] - 32s - loss: 0.0083 - acc: 0.5756 - val_loss: 0.0125 - val_acc: 0.5506\n",
      "Epoch 13/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.5775Epoch 00012: saving model to ./tmp/comma-v3g.12-0.01.hdf5\n",
      "8462/8445 [==============================] - 32s - loss: 0.0081 - acc: 0.5775 - val_loss: 0.0129 - val_acc: 0.5485\n",
      "Epoch 14/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.5746Epoch 00013: saving model to ./tmp/comma-v3g.13-0.01.hdf5\n",
      "8462/8445 [==============================] - 34s - loss: 0.0081 - acc: 0.5748 - val_loss: 0.0136 - val_acc: 0.5479\n",
      "Epoch 15/20\n",
      "8416/8445 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.5749Epoch 00014: saving model to ./tmp/comma-v3g.14-0.01.hdf5\n",
      "8448/8445 [==============================] - 33s - loss: 0.0080 - acc: 0.5747 - val_loss: 0.0137 - val_acc: 0.5443\n",
      "Epoch 16/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.5716Epoch 00015: saving model to ./tmp/comma-v3g.15-0.01.hdf5\n",
      "8462/8445 [==============================] - 33s - loss: 0.0078 - acc: 0.5714 - val_loss: 0.0127 - val_acc: 0.5475\n",
      "Epoch 17/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.5800Epoch 00016: saving model to ./tmp/comma-v3g.16-0.01.hdf5\n",
      "8462/8445 [==============================] - 33s - loss: 0.0076 - acc: 0.5795 - val_loss: 0.0137 - val_acc: 0.5380\n",
      "Epoch 18/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.5751Epoch 00017: saving model to ./tmp/comma-v3g.17-0.01.hdf5\n",
      "8462/8445 [==============================] - 35s - loss: 0.0076 - acc: 0.5753 - val_loss: 0.0128 - val_acc: 0.5401\n",
      "Epoch 19/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.5756Epoch 00018: saving model to ./tmp/comma-v3g.18-0.01.hdf5\n",
      "8462/8445 [==============================] - 35s - loss: 0.0076 - acc: 0.5763 - val_loss: 0.0126 - val_acc: 0.5496\n",
      "Epoch 20/20\n",
      "8430/8445 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.5771Epoch 00019: saving model to ./tmp/comma-v3g.19-0.01.hdf5\n",
      "8462/8445 [==============================] - 35s - loss: 0.0074 - acc: 0.5771 - val_loss: 0.0133 - val_acc: 0.5570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1296bb9e8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "batch_size = 32\n",
    "nb_epoch = 20\n",
    " \n",
    "checkpointer = ModelCheckpoint(filepath=\"./tmp/comma-v3g.{epoch:02d}-{val_loss:.2f}.hdf5\", verbose=1, save_best_only=False)\n",
    "    \n",
    "model.fit_generator(train_generator, \n",
    "                    samples_per_epoch=len(train_samples), \n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=len(validation_samples), nb_epoch=nb_epoch,\n",
    "                    callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model-comma-v3g.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def rgb_to_hsv(image):\n",
    "    # import cv2 \n",
    "    # cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    import tensorflow as tf\n",
    "    return tf.image.rgb_to_hsv(image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create reduced dataset\n",
    "\n",
    "zero = driving_csv[driving_csv[\"steering\"] == 0]\n",
    "print(\"Samples taken out: \", len(zero) // 2)\n",
    "zeros_half = zero.sample(frac=0.5)\n",
    "nonzero = driving_csv[driving_csv[\"steering\"] != 0]\n",
    "reduced_dataset = pd.concat([zeros_half, nonzero])\n",
    "reduced_dataset.to_csv(\"data-udacity/driving_log_reduced.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Flip all of Udacity's data\n",
    "X_path = [driving_csv.iloc[i][\"center\"] \\\n",
    "              for i in range(len(driving_csv))]\n",
    "for filepath in X_path:\n",
    "    cv2.imread('data-udacity/' + filepath)\n",
    "    flipped_image = cv2.flip(input_image,1)\n",
    "    cv2.imwrite('data-udacity-flipped/' + filepath, flipped_image)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Option D: reduced Udacity's dataset\n",
    "\n",
    "## Import data\n",
    "# Added header row manually to CSV.\n",
    "driving_csv = pd.read_csv(\"data-udacity/driving_log.csv\")\n",
    "\n",
    "# Examine data\n",
    "print(\"Number of datapoints: %d\" % len(driving_csv))\n",
    "driving_csv.head()\n",
    "\n",
    "greater_half = driving_csv[driving_csv[\"steering\"] > 0.5]\n",
    "greater_half_neg = driving_csv[driving_csv[\"steering\"] < -0.5]\n",
    "\n",
    "smaller_dataset = pd.concat([greater_half, greater_half_neg])\n",
    "\n",
    "i = -0.5\n",
    "while i < 0.5:\n",
    "    smaller_dataset = pd.concat([smaller_dataset, \n",
    "              driving_csv[driving_csv[\"steering\"] > i].sort_values(by=\"steering\").iloc[0:3]])\n",
    "    i += 0.05\n",
    "print(len(smaller_dataset))\n",
    "\n",
    "# Extract centre image and steering angle from table\n",
    "# Format: X_path: centre image name, y: steeringa angle\n",
    "X_path = [smaller_dataset.iloc[i][\"center\"] \\\n",
    "              for i in range(len(smaller_dataset))]\n",
    "y = [smaller_dataset.iloc[i][\"steering\"] \\\n",
    "              for i in range(len(smaller_dataset))]\n",
    "\n",
    "# Import images\n",
    "X_images = [mpimg.imread(\"data-udacity/\" + image_path) for image_path in X_path]\n",
    "\n",
    "# View image\n",
    "print(\"Images: %d\" % len(X_images))\n",
    "print(\"Sample image\")\n",
    "plt.imshow(X_images[0])\n",
    "# X_images[0]\n",
    "\n",
    "imshape = X_images[0].shape\n",
    "print(\"Image shape: \", imshape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_images, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train model\n",
    "batch_size = 5\n",
    "nb_epoch = 30\n",
    "# data_augmentation = True\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              # validation_split=0.1,\n",
    "              validation_data=(X_val, y_val),\n",
    "              # shuffle=True\n",
    "         )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def flip(image):\n",
    "    import tensorflow as tf\n",
    "    return tf.reverse_sequence(image, [200] * 66, 1, batch_dim=0)\n",
    "\n",
    "def flip_tf(image):\n",
    "    import tensorflow as tf\n",
    "    return tf.image.flip_left_right(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "cropping2d_3 (Cropping2D)        (None, 80, 320, 3)    0           cropping2d_input_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)                (None, 66, 200, 3)    0           cropping2d_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)                (None, 66, 200, 1)    0           lambda_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)                (None, 66, 200, 1)    0           lambda_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 62, 196, 24)   624         lambda_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 62, 196, 24)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_1 (SpatialDropo (None, 62, 196, 24)   0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 58, 192, 36)   21636       spatialdropout2d_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 58, 192, 36)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_2 (SpatialDropo (None, 58, 192, 36)   0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 54, 188, 48)   43248       spatialdropout2d_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 54, 188, 48)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_3 (SpatialDropo (None, 54, 188, 48)   0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 52, 186, 64)   27712       spatialdropout2d_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 52, 186, 64)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_4 (SpatialDropo (None, 52, 186, 64)   0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 50, 184, 64)   36928       spatialdropout2d_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 50, 184, 64)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "spatialdropout2d_5 (SpatialDropo (None, 50, 184, 64)   0           activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 588800)        0           spatialdropout2d_5[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 100)           58880100    flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 100)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 100)           0           activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 50)            5050        dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 50)            0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 50)            0           activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 10)            510         dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 10)            0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 10)            0           activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             11          dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 59015819\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# NVIDIA End to End Learning Pipeline Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Crop the images\n",
    "model.add(Cropping2D(cropping=((60, 20), (0, 0)),\n",
    "                     dim_ordering='tf', # default\n",
    "                     input_shape=(160, 320, 3)))\n",
    "\n",
    "# Resize the data\n",
    "model.add(Lambda(resize))\n",
    "\n",
    "model.add(Lambda(lambda x: x[:,:,:,0:1]))\n",
    "\n",
    "# Normalise data\n",
    "# TODO: some people use /255.0 - 0.5. Why?\n",
    "model.add(Lambda(lambda x: x/127.5 - 1.))\n",
    "\n",
    "\n",
    "# Conv layer 1, 5x5 kernel to 24@ (from 3@)\n",
    "# TODO: What does the number of filters MEAN?\n",
    "# Stride of 2x2\n",
    "model.add(Convolution2D(24, 5, 5,\n",
    "                        input_shape=(66,200))\n",
    "         )\n",
    "\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 2, 5x5 kernel to 36@\n",
    "model.add(Convolution2D(36, 5, 5))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 3, 5x5 kernel to 48@\n",
    "model.add(Convolution2D(48, 5, 5))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 4, 3x3 kernel to 64@\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Conv layer 5, 3x3 kernel to 64@\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('elu'))\n",
    "model.add(SpatialDropout2D(0.1))\n",
    "\n",
    "# Flatten\n",
    "model.add(Flatten())\n",
    "\n",
    "# Removed: fully connected layer 1, 1164 neurons\n",
    "\n",
    "# Fc2, 100 neurons\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Fc3, 50 neurons\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Fc4, 10 neurons\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Output\n",
    "model.add(Dense(1, activation=\"tanh\"))\n",
    "\n",
    "# Compile model\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
